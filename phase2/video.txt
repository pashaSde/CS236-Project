# DATABASE LOADER VIDEO SCRIPT (2.5 minutes)

---

## [0:00-0:20] INTRODUCTION & DATABASE SETUP

Hello, I'm presenting Phase 2 of our hotel booking database project. We're loading three 
datasets into PostgreSQL using PySpark.

First, we set up PostgreSQL using Docker. This creates the 'innsight' 
database running on port 5432.

---

## [0:20-0:50] SCHEMA DESIGN

Our schema design uses a unified 14-column structure for all three tables: 
customer reservations, hotel bookings, and merged data.

Key design decisions: We use VARCHAR for booking_id to preserve 
the INN prefix, BOOLEAN for is_canceled for efficiency, and nullable fields 
like hotel and country since not all datasets have these.

We created indexes on frequently queried columns: price, 
cancellation status, dates, and market segments for 3 to 6x faster filtering.

---

## [0:50-1:30] CODE WALKTHROUGH

Let's look at the PySpark implementation. The script 
has several key functions:

The normalize_booking_status function handles 
inconsistent data. Customer reservations use 'Canceled' and 'Not_Canceled', while hotel 
bookings use 'true' and 'false'. We convert all variations to proper boolean values.

The load_table function reads CSVs, applies 
transformations, and writes to PostgreSQL via JDBC. Mode overwrite allows
 us to re-run the script safely.

Finally, create_indexes adds performance 
indexes on four columns per table and runs ANALYZE to update table statistics.

---

## [1:30-2:10] EXECUTION DEMO

Now let's run the loader. 
First, I set the database password environment variable, then execute with spark-submit.

The script loads each dataset...
 customer reservations, 36,275 rows... hotel bookings, 78,702 rows... merged data, 
 114,977 rows. Now creating indexes... done!

All three tables loaded successfully with indexes created.

---

## [2:10-2:30] VERIFICATION & WRAP-UP

Verification shows three tables with correct row 
counts and 12 indexes created. Our database is now populated and optimized, ready for 
Phase 3 analysis and the WebUI.

Thank you!